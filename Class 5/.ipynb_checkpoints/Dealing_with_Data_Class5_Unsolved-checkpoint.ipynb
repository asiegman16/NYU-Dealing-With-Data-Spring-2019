{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Data Spring 2020 â€“ Class 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files and Printing\n",
    "\n",
    "Often you will need to read data from a file, or write the output of a Python script back to a file. \n",
    "\n",
    "We use the `open` function to open the file in the appropriate mode, which takes two arguments: \n",
    "\n",
    "1. the name of the file,\n",
    "2. and the mode. \n",
    "\n",
    "> `a_file = open(filename, mode)`\n",
    "\n",
    "The `mode` is a single letter string that specifies if you are going to be reading from a file, writing to a file, or appending to the end of an existing file. The modes are: \n",
    "\n",
    "+ `'r'` : open a file for reading\n",
    "+ `'w'` : open a file for writing (beware, this will overwrite any previously existing file) \n",
    "+ `'a'` : append (write to the end of a file) \n",
    "\n",
    "When reading a file, you usually want to iterate through the lines in that file using a `for loop`. Some other common methods for dealing with files are: \n",
    "\n",
    "+ `file.read()` : read the entire contents of a file into a string\n",
    "+ `file.write(some_string)` : writes to the file (note, this doesn't automatically include new lines) \n",
    "+ `file.flush()` : write out any buffered writes\n",
    "+ `file.close()` : close the open file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing a file to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the file temp.txt, and get it ready for writing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading a file from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we read the file, we have the lines in a big string. Let's process that big string a little bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the file in the cell above and split the content of the file using the newline character '\\n'\n",
    "\n",
    "\n",
    "\n",
    "# iterate through the line variable (it is a list of strings) and then print the length of each line\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a file numbers.txt and write the numbers from 0 to 24 there\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now open the file for reading\n",
    "\n",
    "\n",
    "\n",
    "# and read the full content of the file in memory, as a big string\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we convert the strings into integers\n",
    "# we use the conditional to avoid trying to parse the string '' that is at the end of the list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's clean up\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python `os` standard library\n",
    "Another addition to our file handling toolkit is the `os` library which provides ways to move files, make directories, and gather data about the file system. Like other standard libraries, we need to import it to use it via `import os`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first - append the new folder name and create a file path string using os.path.sep and the string.join() method\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# second - create the directory using the file path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# third - add the file name \"tmp2.txt\" to the path\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now open the file for writing using the absolute path \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Putting our Python to work: Exploring and Cleaning the Census Data file**\n",
    "\n",
    "So far in our class project, we have identified three data files (IRS tax return counts for NYC, US census data for NYC, and NYC film permits data) and we have skipped over the cleaning step to transform them in SQL.\n",
    "\n",
    "Now, we'll take one step back and begin cleaning them, starting with the smallest file - the US Census data.\n",
    "\n",
    "Our approach will be as follows:\n",
    "1. Inspect the data thoroughly to understand its benefits and risks for processing, and what information lies within\n",
    "* Clean/fix the data to remove any issues that would prevent it from working with SQL, such as weird characters, too many columns, missing data, splitting values into multiple rows, or combining multiple values into a single row\n",
    "* Structure the data found in the file as a Python native structure so we can manipulate and prepare it for use in SQL\n",
    "* Migrate our approach to a dedicated Python file\n",
    "\n",
    "We'll use: file read/write, loops, nested structures and UDFs to do all of this. (Some will be homework.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1) Inspect our Data**\n",
    "First - we need to understand the data in this file - using either Excel or JupyterLab's CSV reader.\n",
    "\n",
    "Upon inspection (either way, here's what we learn:\n",
    "* there are 350+ columns\n",
    "* there are zipcodes for just NYC \n",
    "* there are a mix of letter and numbers for values\n",
    "* we have both percent and numbers values\n",
    "* it appears to be comma-separated\n",
    "* there are two column header lines: one with codes, and another with human-readable labels\n",
    "\n",
    "Next - we inspect with Python in two ways: first we'll look at the raw file as a string, then we'll probe whether the comma-separation is fail safe or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work in Google Colab, we must manually upload our CSV into the environment..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 500 characters of the census file\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and the last 500 characters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Review Comma-Based Separability**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first 500 characters, we see that the census has \"code labels\" for each column, none of which have funky characters that could trip us up with comma separation. \n",
    "\n",
    "We need to work with just that first line, so we'll use readline (which reads one entire line from the file) on the file handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's separate that first line into a list and see exactly how many columns we have\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We knew it had 350+ and this is too many columns to effectively work with. \n",
    "\n",
    "We know from our transform work together that we need to weight up/down the data, which means percentages won't be useful, so we can remove those. \n",
    "\n",
    "We can also remove some of the family occupancy data because it was decided to focus on gender, income and ethnicity in the project.\n",
    "\n",
    "Before we get to removing data, though, we need to put our data into a structure we can slice and dice. In other words, we need to transform our \"string\" data into a list of lists - a nested data structure where each row is a list, and within that list, each column is a list - a nested data structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example: let's look at this simple 3x3 table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2) Cleaning Up Wonky Data**\n",
    "\n",
    "But before we can even create a nested structure - we need to be confident we can split the data correctly for every single line.\n",
    "\n",
    "Unfortunately, CSV data is known to be particularly tricky because sometimes data sources use commas in column labels but surround those column headers with \"\" because Excel will treat it right. \n",
    "\n",
    "Python won't be so forgiving so we need to test for \"\" in the data - we'll do this using a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh-oh! those quotes spell trouble so now we need to see where that first quote appears and if a comma appears after it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We suspect that those \"\"  in human-readable column headers of the second line of data are hiding \"...**,**...\" and will cause any nesting using comma-separation to create extra columns.\n",
    "\n",
    "Let's prove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to isolate the second line using \"find\": \n",
    "# the second line is between the first and second \\n\n",
    "\n",
    "\n",
    "\n",
    "# split the second line using commas to see how many columns we get\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've proven those will be a problem, we are going to use Python to clean those up via our own User Defined Function for this purpose, because it may appear in another data source, too.\n",
    "\n",
    "Before we create the UDF, let's describe what we want our function to do: \n",
    "\n",
    "1. it will accept a string input\n",
    "2. it will remove \" characters from the input\n",
    "3. when it finds a , between \"\" it will replace , characters in the input with a `-`. However, it will NOT change `,` otherwise since it is a CSV file.\n",
    "4. it will return a string output\n",
    "\n",
    "And, we will create a test_input and expected_output to test our UDF during development:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our test data\n",
    "\n",
    "\n",
    "\n",
    "test_output == exp_output # compare UDF run to expected output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we try our second_line variable storing the problem row for cleaning \n",
    "# proof it works: no \" and len after CSV split = 375\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works! Now, let's fix our original input data string, `census_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3) Creating a Nested Structure**\n",
    "\n",
    "Now that we've cleaned up the wonkiness in the data, we can create our nested structure using a `for` loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by reviewing our simple 3x3 table example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example, the data is structured as:\n",
    "\n",
    "`list[row][column]`\n",
    "\n",
    "And we will use split commands to do the same for our data, this time creating another UDF.\n",
    "\n",
    "Again - as before - let's state what it will do and create test data:\n",
    "1. the UDF will take a data string as input\n",
    "* it will create a list where each line in the data, identified using `\\n`, is an item\n",
    "* for each item in the first list `list[row]`, it will create a list of columns, using comma-separatiuon (`,`) to identify each item\n",
    "* it would be nice for the UDF user to specify what character separates rows and columns, separately\n",
    "* the UDF will return the nested data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a dataset string where each row is separated by input_row_delim and each column is separate by \n",
    "# input_col_delim to create a nested object of lists\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a nested structure of our census data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4) Moving from Notebooks to \\*.py Files** \n",
    "\n",
    "But before you go to do that, we need to start moving the findings of our exploration into a dedicated python file for cleaning the Census data. \n",
    "\n",
    "We're making this migration because notebooks are great for exploring data, but as our files and project grow larger, it is simpler to run the Python files outside notebooks AND sometimes very large files can cause our notebooks to crash.\n",
    "\n",
    "Here's what that new Python file needs to do:\n",
    "\n",
    "1. read the census data file into a variable\n",
    "2. clean the data by removing the \"...,...\" problem using a UDF\n",
    "3. create a nested data structure\n",
    "4. remove unwanted columns\n",
    "5. create a new string from the nested structure\n",
    "6. write the file to disk\n",
    "\n",
    "We've already #1, #2, #3, and #6 together, so we'll isolate those below along with comments to do the other work before putting in its own file.\n",
    "\n",
    "And recall that our UDFs has to be put ahead of the main program to work because the Main Program needs to know what happens in those UDFs.\n",
    "\n",
    "Below is the exact code that will be put into its own file named `clean_census.py` and we'll run it together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
