{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Data Spring 2020 â€“ Class 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files and Printing\n",
    "\n",
    "Often you will need to read data from a file, or write the output of a Python script back to a file. \n",
    "\n",
    "We use the `open` function to open the file in the appropriate mode, which takes two arguments: \n",
    "\n",
    "1. the name of the file,\n",
    "2. and the mode. \n",
    "\n",
    "> `a_file = open(filename, mode)`\n",
    "\n",
    "The `mode` is a single letter string that specifies if you are going to be reading from a file, writing to a file, or appending to the end of an existing file. The modes are: \n",
    "\n",
    "+ `'r'` : open a file for reading\n",
    "+ `'w'` : open a file for writing (beware, this will overwrite any previously existing file) \n",
    "+ `'a'` : append (write to the end of a file) \n",
    "\n",
    "When reading a file, you usually want to iterate through the lines in that file using a `for loop`. Some other common methods for dealing with files are: \n",
    "\n",
    "+ `file.read()` : read the entire contents of a file into a string\n",
    "+ `file.write(some_string)` : writes to the file (note, this doesn't automatically include new lines) \n",
    "+ `file.flush()` : write out any buffered writes\n",
    "+ `file.close()` : close the open file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing a file to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the file temp.txt, and get it ready for writing\n",
    "\n",
    "f = open(\"temp.txt\", \"w\")\n",
    "f.write(\"This is my first file! The end!\\n\")\n",
    "f.write(\"Oh wait, I wanted to say something else.\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the command below is one of the IPython \"magics\" - commands within the notebook unrelated to python\n",
    "# %magic shows you the list of basic commands and %lsmagic shows you all the super commands\n",
    "\n",
    "# for more info, check out https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/\n",
    "\n",
    "%more temp.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading a file from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the file for reading\n",
    "\n",
    "f = open(\"temp.txt\", \"r\")\n",
    "content = f.read() # read the full content of the file in memory, as a big string\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is my first file! The end!\\nOh wait, I wanted to say something else.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we read the file, we have the lines in a big string. Let's process that big string a little bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is my first file! The end!  ===>  31\n",
      "Oh wait, I wanted to say something else.  ===>  40\n"
     ]
    }
   ],
   "source": [
    "# read the file in the cell above and split the content of the file using the newline character '\\n'\n",
    "\n",
    "lines = content.split(\"\\n\")\n",
    "\n",
    "# iterate through the line variable (it is a list of strings) and then print the length of each line\n",
    "\n",
    "for line in lines:\n",
    "    print(line, \" ===> \", len(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a file numbers.txt and write the numbers from 0 to 24 there\n",
    "\n",
    "f = open(\"numbers.txt\", \"w\")\n",
    "\n",
    "for num in range(25):\n",
    "    f.write(str(num)+'\\n')\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "%more numbers.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now open the file for reading\n",
    "\n",
    "f = open(\"numbers.txt\", \"r\")\n",
    "\n",
    "# and read the full content of the file in memory, as a big string\n",
    "\n",
    "content = f.read()\n",
    "\n",
    "f.close()\n",
    "\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n"
     ]
    }
   ],
   "source": [
    "# here we convert the strings into integers\n",
    "# we use the conditional to avoid trying to parse the string '' that is at the end of the list\n",
    "\n",
    "numbers = []\n",
    "lines = content.split(\"\\n\")\n",
    "\n",
    "for line in lines:\n",
    "    if len(line) > 0:\n",
    "        numbers.append(int(line))\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "print(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's clean up\n",
    "\n",
    "# windows\n",
    "#!del temp.txt\n",
    "#!del numbers.txt\n",
    "\n",
    "# macOS\n",
    "!rm temp.txt\n",
    "!rm numbers.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python `os` standard library\n",
    "Another addition to our file handling toolkit is the `os` library which provides ways to move files, make directories, and gather data about the file system. Like other standard libraries, we need to import it to use it via `import os`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/siegmanA/Desktop/NYU-Dealing-With-Data-Spring-2020/Class 5'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# let's get info about our current working directory - the folder our Python applications are working in\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dealing_with_Data_Class5_Solved.ipynb',\n",
       " '.DS_Store',\n",
       " 'raw_census_2010.csv',\n",
       " '.ipynb_checkpoints',\n",
       " '[YOUR_NAME_HERE]_Class5_Homework.ipynb',\n",
       " 'Class5_Homework_Solution.ipynb']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# next, let's list everything in the directory\n",
    "\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'siegmanA'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ... and see what our login name is\n",
    "\n",
    "os.getlogin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .. and find out the \"separate\" used in constructing file paths\n",
    "# every operating system is different, and this value enables your python to be cross-platform \n",
    "\n",
    "os.path.sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'Users', 'siegmanA', 'Desktop', 'NYU-Dealing-With-Data-Spring-2020', 'Class 5']\n"
     ]
    }
   ],
   "source": [
    "# ... now we can create our own paths for new files - important for creating a \"clean\" version of source data\n",
    "\n",
    "dir_list = os.getcwd().split(os.path.sep)\n",
    "print(dir_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/siegmanA/Desktop/NYU-Dealing-With-Data-Spring-2020/Class 5/Class 5 - tmp/tmp2.txt\n"
     ]
    }
   ],
   "source": [
    "# ... now let's create an output file in a new sub-folder called Class 7 - tmp\n",
    "\n",
    "# first - append the new folder name and create a file path string using os.path.sep and the string.join() method\n",
    "\n",
    "dir_list.append(\"Class 5 - tmp\")\n",
    "dir_string = os.path.sep.join(dir_list)\n",
    "\n",
    "# second - create the directory using the file path\n",
    "\n",
    "os.mkdir(dir_string)\n",
    "\n",
    "# third - add the file name \"tmp2.txt\" to the path\n",
    "\n",
    "dir_list.append(\"tmp2.txt\")\n",
    "dir_string = os.path.sep.join(dir_list)\n",
    "\n",
    "print(dir_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now open the file for writing using the absolute path \n",
    "# *we could have also used os.chdir(\"7-tmp\") and open(\"tmp2.txt\") but it's better to use absolute paths\n",
    "\n",
    "file_handle = open(dir_string,\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_handle.write(\"test file\\nsecond line\\n\")\n",
    "file_handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up \n",
    "\n",
    "os.chdir(\"/Users/siegmanA/Desktop/NYU-Dealing-With-Data-Spring-2020/Class 5/Class 5 - tmp\")\n",
    "!rm \"tmp2.txt\"\n",
    "\n",
    "os.chdir(\"..\")\n",
    "!rmdir \"Class 5 - tmp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Putting our Python to work: Exploring and Cleaning the Census Data file**\n",
    "\n",
    "So far in our class project, we have identified three data files (IRS tax return counts for NYC, US census data for NYC, and NYC film permits data) and we have skipped over the cleaning step to transform them in SQL.\n",
    "\n",
    "Now, we'll take one step back and begin cleaning them, starting with the smallest file - the US Census data.\n",
    "\n",
    "Our approach will be as follows:\n",
    "1. Inspect the data thoroughly to understand its benefits and risks for processing, and what information lies within\n",
    "* Clean/fix the data to remove any issues that would prevent it from working with SQL, such as weird characters, too many columns, missing data, splitting values into multiple rows, or combining multiple values into a single row\n",
    "* Structure the data found in the file as a Python native structure so we can manipulate and prepare it for use in SQL\n",
    "* Migrate our approach to a dedicated Python file\n",
    "\n",
    "We'll use: file read/write, loops, nested structures and UDFs to do all of this. (Some will be homework.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1) Inspect our Data**\n",
    "First - we need to understand the data in this file - using either Excel or JupyterLab's CSV reader.\n",
    "\n",
    "Upon inspection (either way, here's what we learn:\n",
    "* there are 350+ columns\n",
    "* there are zipcodes for just NYC \n",
    "* there are a mix of letter and numbers for values\n",
    "* we have both percent and numbers values\n",
    "* it appears to be comma-separated\n",
    "* there are two column header lines: one with codes, and another with human-readable labels\n",
    "\n",
    "Next - we inspect with Python in two ways: first we'll look at the raw file as a string, then we'll probe whether the comma-separation is fail safe or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/siegmanA/Desktop/NYU-Dealing-With-Data-Spring-2020/Class 5'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# REVIEWING DATA AS A STRING\n",
    "\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the location of the raw_census_2010.csv file you downloaded and open it for reading into a variable\n",
    "\n",
    "file_handle = open('/Users/siegmanA/Desktop/NYU-Dealing-With-Data-Spring-2020/Class 5/raw_census_2010.csv',\"r\")\n",
    "census_data = file_handle.read()\n",
    "file_handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GEO.id,GEO.id2,GEO.display-label,HD01_S001,HD02_S001,HD01_S002,HD02_S002,HD01_S003,HD02_S003,HD01_S004,HD02_S004,HD01_S005,HD02_S005,HD01_S006,HD02_S006,HD01_S007,HD02_S007,HD01_S008,HD02_S008,HD01_S009,HD02_S009,HD01_S010,HD02_S010,HD01_S011,HD02_S011,HD01_S012,HD02_S012,HD01_S013,HD02_S013,HD01_S014,HD02_S014,HD01_S015,HD02_S015,HD01_S016,HD02_S016,HD01_S017,HD02_S017,HD01_S018,HD02_S018,HD01_S019,HD02_S019,HD01_S020,HD02_S020,HD01_S021,HD02_S021,HD01_S022,HD02_S022,HD01_S023,HD02_S023,HD01_S0'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the first 500 characters of the census file\n",
    "\n",
    "census_data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4,44.4,1802,19.9,2373,26.2,1878,20.7,281,3.1,104,1.1,59,0.7,508,5.6,49,0.5,33,0.4,297,3.3,82,0.9,47,0.5,19,0.2,28,0.3,35,0.4,10,0.1,25,0.3,4024,100.0,2434,60.5,1035,25.7,1802,44.8,647,16.1,172,4.3,98,2.4,460,11.4,290,7.2,1590,39.5,1331,33.1,507,12.6,131,3.3,824,20.5,403,10.0,1120,27.8,1169,29.1,2.23, ( X ) ,2.83, ( X ) ,4326,100.0,4024,93.0,302,7.0,75,1.7,5,0.1,49,1.1,24,0.6,32,0.7,117,2.7,1.8, ( X ) ,5.4, ( X ) ,4024,100.0,2726,67.7,6370, ( X ) ,2.34, ( X ) ,1298,32.3,2618, ( X ) ,2.02, ( X ) \\n'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and the last 500 characters\n",
    "census_data[-500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Review Comma-Based Separability**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first 500 characters, we see that the census has \"code labels\" for each column, none of which have funky characters that could trip us up with comma separation. \n",
    "\n",
    "We need to work with just that first line, so we'll use readline (which reads one entire line from the file) on the file handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GEO.id,GEO.id2,GEO.display-label,HD01_S001,HD02_S001,HD01_S002,HD02_S002,HD01_S003,HD02_S003,HD01_S004,HD02_S004,HD01_S005,HD02_S005,HD01_S006,HD02_S006,HD01_S007,HD02_S007,HD01_S008,HD02_S008,HD01_S009,HD02_S009,HD01_S010,HD02_S010,HD01_S011,HD02_S011,HD01_S012,HD02_S012,HD01_S013,HD02_S013,HD01_S014,HD02_S014,HD01_S015,HD02_S015,HD01_S016,HD02_S016,HD01_S017,HD02_S017,HD01_S018,HD02_S018,HD01_S019,HD02_S019,HD01_S020,HD02_S020,HD01_S021,HD02_S021,HD01_S022,HD02_S022,HD01_S023,HD02_S023,HD01_S024,HD02_S024,HD01_S025,HD02_S025,HD01_S026,HD02_S026,HD01_S027,HD02_S027,HD01_S028,HD02_S028,HD01_S029,HD02_S029,HD01_S030,HD02_S030,HD01_S031,HD02_S031,HD01_S032,HD02_S032,HD01_S033,HD02_S033,HD01_S034,HD02_S034,HD01_S035,HD02_S035,HD01_S036,HD02_S036,HD01_S037,HD02_S037,HD01_S038,HD02_S038,HD01_S039,HD02_S039,HD01_S040,HD02_S040,HD01_S041,HD02_S041,HD01_S042,HD02_S042,HD01_S043,HD02_S043,HD01_S044,HD02_S044,HD01_S045,HD02_S045,HD01_S046,HD02_S046,HD01_S047,HD02_S047,HD01_S048,HD02_S048,HD01_S049,HD02_S049,HD01_S050,HD02_S050,HD01_S051,HD02_S051,HD01_S052,HD02_S052,HD01_S053,HD02_S053,HD01_S054,HD02_S054,HD01_S055,HD02_S055,HD01_S056,HD02_S056,HD01_S057,HD02_S057,HD01_S058,HD02_S058,HD01_S059,HD02_S059,HD01_S060,HD02_S060,HD01_S061,HD02_S061,HD01_S062,HD02_S062,HD01_S063,HD02_S063,HD01_S064,HD02_S064,HD01_S065,HD02_S065,HD01_S066,HD02_S066,HD01_S067,HD02_S067,HD01_S068,HD02_S068,HD01_S069,HD02_S069,HD01_S070,HD02_S070,HD01_S071,HD02_S071,HD01_S072,HD02_S072,HD01_S073,HD02_S073,HD01_S074,HD02_S074,HD01_S075,HD02_S075,HD01_S076,HD02_S076,HD01_S077,HD02_S077,HD01_S078,HD02_S078,HD01_S079,HD02_S079,HD01_S080,HD02_S080,HD01_S081,HD02_S081,HD01_S082,HD02_S082,HD01_S083,HD02_S083,HD01_S084,HD02_S084,HD01_S085,HD02_S085,HD01_S086,HD02_S086,HD01_S087,HD02_S087,HD01_S088,HD02_S088,HD01_S089,HD02_S089,HD01_S090,HD02_S090,HD01_S091,HD02_S091,HD01_S092,HD02_S092,HD01_S093,HD02_S093,HD01_S094,HD02_S094,HD01_S095,HD02_S095,HD01_S096,HD02_S096,HD01_S097,HD02_S097,HD01_S098,HD02_S098,HD01_S099,HD02_S099,HD01_S100,HD02_S100,HD01_S101,HD02_S101,HD01_S102,HD02_S102,HD01_S103,HD02_S103,HD01_S104,HD02_S104,HD01_S105,HD02_S105,HD01_S106,HD02_S106,HD01_S107,HD02_S107,HD01_S108,HD02_S108,HD01_S109,HD02_S109,HD01_S110,HD02_S110,HD01_S111,HD02_S111,HD01_S112,HD02_S112,HD01_S113,HD02_S113,HD01_S114,HD02_S114,HD01_S115,HD02_S115,HD01_S116,HD02_S116,HD01_S117,HD02_S117,HD01_S118,HD02_S118,HD01_S119,HD02_S119,HD01_S120,HD02_S120,HD01_S121,HD02_S121,HD01_S122,HD02_S122,HD01_S123,HD02_S123,HD01_S124,HD02_S124,HD01_S125,HD02_S125,HD01_S126,HD02_S126,HD01_S127,HD02_S127,HD01_S128,HD02_S128,HD01_S129,HD02_S129,HD01_S130,HD02_S130,HD01_S131,HD02_S131,HD01_S132,HD02_S132,HD01_S133,HD02_S133,HD01_S134,HD02_S134,HD01_S135,HD02_S135,HD01_S136,HD02_S136,HD01_S137,HD02_S137,HD01_S138,HD02_S138,HD01_S139,HD02_S139,HD01_S140,HD02_S140,HD01_S141,HD02_S141,HD01_S142,HD02_S142,HD01_S143,HD02_S143,HD01_S144,HD02_S144,HD01_S145,HD02_S145,HD01_S146,HD02_S146,HD01_S147,HD02_S147,HD01_S148,HD02_S148,HD01_S149,HD02_S149,HD01_S150,HD02_S150,HD01_S151,HD02_S151,HD01_S152,HD02_S152,HD01_S153,HD02_S153,HD01_S154,HD02_S154,HD01_S155,HD02_S155,HD01_S156,HD02_S156,HD01_S157,HD02_S157,HD01_S158,HD02_S158,HD01_S159,HD02_S159,HD01_S160,HD02_S160,HD01_S161,HD02_S161,HD01_S162,HD02_S162,HD01_S163,HD02_S163,HD01_S164,HD02_S164,HD01_S165,HD02_S165,HD01_S166,HD02_S166,HD01_S167,HD02_S167,HD01_S168,HD02_S168,HD01_S169,HD02_S169,HD01_S170,HD02_S170,HD01_S171,HD02_S171,HD01_S172,HD02_S172,HD01_S173,HD02_S173,HD01_S174,HD02_S174,HD01_S175,HD02_S175,HD01_S176,HD02_S176,HD01_S177,HD02_S177,HD01_S178,HD02_S178,HD01_S179,HD02_S179,HD01_S180,HD02_S180,HD01_S181,HD02_S181,HD01_S182,HD02_S182,HD01_S183,HD02_S183,HD01_S184,HD02_S184,HD01_S185,HD02_S185,HD01_S186,HD02_S186\\n'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_handle = open('/Users/siegmanA/Desktop/NYU-Dealing-With-Data-Spring-2020/Class 5/raw_census_2010.csv',\"r\")\n",
    "census_first_line = file_handle.readline()\n",
    "file_handle.close()\n",
    "census_first_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's separate that first line into a list and see exactly how many columns we have\n",
    "\n",
    "header1_list = census_first_line.split(\",\")\n",
    "len(header1_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We knew it had 350+ and this is too many columns to effectively work with. \n",
    "\n",
    "We know from our transform work together that we need to weight up/down the data, which means percentages won't be useful, so we can remove those. \n",
    "\n",
    "We can also remove some of the family occupancy data because it was decided to focus on gender, income and ethnicity in the project.\n",
    "\n",
    "Before we get to removing data, though, we need to put our data into a structure we can slice and dice. In other words, we need to transform our \"string\" data into a list of lists - a nested data structure where each row is a list, and within that list, each column is a list - a nested data structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example: let's look at this simple 3x3 table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c']\n",
      "[123, 456, 789]\n",
      "[987, 654, 321]\n",
      "row 1, column 3 ==> c\n",
      "row 2, column 2 ==> 456\n",
      "row 3, column 1 ==> 987\n"
     ]
    }
   ],
   "source": [
    "example_list = [ \n",
    "    [\"a\",\"b\",\"c\"],\n",
    "    [123,456,789],\n",
    "    [987,654,321]\n",
    "    ]\n",
    "\n",
    "print(example_list[0])\n",
    "print(example_list[1])\n",
    "print(example_list[2])\n",
    "\n",
    "print(\"row 1, column 3 ==>\",example_list[0][2])\n",
    "print(\"row 2, column 2 ==>\",example_list[1][1])\n",
    "print(\"row 3, column 1 ==>\",example_list[2][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2) Cleaning Up Wonky Data**\n",
    "\n",
    "But before we can even create a nested structure - we need to be confident we can split the data correctly for every single line.\n",
    "\n",
    "Unfortunately, CSV data is known to be particularly tricky because sometimes data sources use commas in column labels but surround those column headers with \"\" because Excel will treat it right. \n",
    "\n",
    "Python won't be so forgiving so we need to test for \"\" in the data - we'll do this using a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "# using a for loop\n",
    "\n",
    "quote_count = 0\n",
    "\n",
    "for char in census_data:\n",
    "    if char == '\"':\n",
    "        quote_count += 1\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "print(quote_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh-oh! those quotes spell trouble so now we need to see where that first quote appears and if a comma appears after it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 18 years,\"Number; HOUSEHOLDS BY TYPE - Total households - Family households (families) [7] - Male householder, no wife present\",\"'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quote_pos = census_data.find('\"')\n",
    "comma_pos = census_data.find(\",\",quote_pos)\n",
    "census_data[quote_pos - 10: comma_pos + 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We suspect that those \"\"  in human-readable column headers of the second line of data are hiding \"...**,**...\" and will cause any nesting using comma-separation to create extra columns.\n",
    "\n",
    "Let's prove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "391"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to isolate the second line using \"find\": \n",
    "# the second line is between the first and second \\n\n",
    "\n",
    "first_nl = census_data.find(\"\\n\")\n",
    "second_nl = census_data.find(\"\\n\",first_nl+1)\n",
    "second_line = census_data[first_nl+1:second_nl]\n",
    "\n",
    "# split the second line using commas to see how many columns we get\n",
    "\n",
    "len(second_line.split(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've proven those will be a problem, we are going to use Python to clean those up via our own User Defined Function for this purpose, because it may appear in another data source, too.\n",
    "\n",
    "Before we create the UDF, let's describe what we want our function to do: \n",
    "\n",
    "1. it will accept a string input\n",
    "2. it will remove \" characters from the input\n",
    "3. when it finds a , between \"\" it will replace , characters in the input with a `-`. However, it will NOT change `,` otherwise since it is a CSV file.\n",
    "4. it will return a string output\n",
    "\n",
    "And, we will create a test_input and expected_output to test our UDF during development:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unquotable(input_string): # will remove those , within \"\" before changing , to \\t \n",
    "    \n",
    "    tmp_list = input_string.split('\"') # remove \" char by splitting the input string into a list using that char\n",
    "\n",
    "    # remove , char\n",
    "    for i in range(len(tmp_list)):\n",
    "        # skip items that start/end with commas as these aren't quoted items & we don't want to remove their commas \n",
    "        if (tmp_list[i][0] == \",\") or (tmp_list[i][-1] == \",\"):\n",
    "            continue\n",
    "        else: # replace , with - in quoted items\n",
    "            tmp_list[i] = tmp_list[i].replace(\",\",\"-\")\n",
    "\n",
    "    output = \",\".join(tmp_list) # rejoin items into a string\n",
    "\n",
    "    # get rid of any \",,\" and \",,,\" that could exist as it will add extra columns\n",
    "    output = output.replace(\",,,\",\",\")\n",
    "    output = output.replace(\",,\",\",\")\n",
    "    \n",
    "    return output # return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our test data\n",
    "\n",
    "test_input = 'something,\"test string: a,b,c\",other thing'\n",
    "exp_output = 'something,test string: a-b-c,other thing'\n",
    "\n",
    "test_output = unquotable(test_input) # our testing lines - run the UDF\n",
    "\n",
    "test_output == exp_output # compare UDF run to expected output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "375\n"
     ]
    }
   ],
   "source": [
    "# now we try our second_line variable storing the problem row for cleaning \n",
    "# proof it works: no \" and len after CSV split = 375\n",
    "\n",
    "new_row = unquotable(second_line)\n",
    "\n",
    "print('\"' in new_row)\n",
    "print(len(new_row.split(\",\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works! Now, let's fix our original input data string, `census_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_census_data = unquotable(census_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3) Creating a Nested Structure**\n",
    "\n",
    "Now that we've cleaned up the wonkiness in the data, we can create our nested structure using a `for` loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by reviewing our simple 3x3 table example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c']\n",
      "[123, 456, 789]\n",
      "[987, 654, 321]\n",
      "row 1, column 3 ==> c\n",
      "row 2, column 2 ==> 456\n",
      "row 3, column 1 ==> 987\n"
     ]
    }
   ],
   "source": [
    "example_list = [ \n",
    "    [\"a\",\"b\",\"c\"],\n",
    "    [123,456,789],\n",
    "    [987,654,321]\n",
    "    ]\n",
    "\n",
    "print(example_list[0])\n",
    "print(example_list[1])\n",
    "print(example_list[2])\n",
    "\n",
    "print(\"row 1, column 3 ==>\",example_list[0][2])\n",
    "print(\"row 2, column 2 ==>\",example_list[1][1])\n",
    "print(\"row 3, column 1 ==>\",example_list[2][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example, the data is structured as:\n",
    "\n",
    "`list[row][column]`\n",
    "\n",
    "And we will use split commands to do the same for our data, this time creating another UDF.\n",
    "\n",
    "Again - as before - let's state what it will do and create test data:\n",
    "1. the UDF will take a data string as input\n",
    "* it will create a list where each line in the data, identified using `\\n`, is an item\n",
    "* for each item in the first list `list[row]`, it will create a list of columns, using comma-separatiuon (`,`) to identify each item\n",
    "* it would be nice for the UDF user to specify what character separates rows and columns, separately\n",
    "* the UDF will return the nested data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@    \n",
    "#@@@@@@@@ UDF nester() @@@@@@@@@\n",
    "\n",
    "# take a dataset string where each row is separated by input_row_delim and each column is separate by \n",
    "# input_col_delim to create a nested object of lists\n",
    "\n",
    "def nester(input_string,input_row_delim,input_col_delim):\n",
    "\n",
    "    row_list = input_string.split(input_row_delim) # create a list item for each row in the file using the row delimiter\n",
    "\n",
    "    nested_data = [] #output var\n",
    "\n",
    "    # created nested structure to store each column separately list of rows where each row is a list of columns)\n",
    "    for i in range(len(row_list)): \n",
    "        row = row_list[i]\n",
    "        col = row.split(input_col_delim)\n",
    "        nested_data.append(col)\n",
    "    \n",
    "    return nested_data # return the nested structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our test data\n",
    "\n",
    "test_input = 'r1c1,r1c2,r1c3\\nr2c1,r2c2,r2c3\\nr3c1,r3c2,r3c3'\n",
    "exp_output = [\n",
    "    ['r1c1','r1c2','r1c3'],\n",
    "    ['r2c1','r2c2','r2c3'],\n",
    "    ['r3c1','r3c2','r3c3']\n",
    "    ]\n",
    "\n",
    "test_output = nester(test_input,'\\n',',') # our testing - run the UDF\n",
    "\n",
    "test_output == exp_output # compare UDF output to expected output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a nested structure of our census data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_struc = nester(clean_census_data,'\\n',',') # structure our original string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8600000US06390', '06390', 'ZCTA5 06390', '236', '100.0', '6', '2.5', '9', '3.8', '16', '6.8', '11', '4.7', '7', '3.0', '7', '3.0', '7', '3.0', '13', '5.5', '20', '8.5', '27', '11.4', '22', '9.3', '29', '12.3', '19', '8.1', '20', '8.5', '6', '2.5', '6', '2.5', '8', '3.4', '3', '1.3', '49.4', ' ( X ) ', '200', '84.7', '196', '83.1', '193', '81.8', '55', '23.3', '43', '18.2', '118', '50.0', '2', '0.8', '7', '3.0', '8', '3.4', '4', '1.7', '4', '1.7', '3', '1.3', '2', '0.8', '7', '3.0', '9', '3.8', '17', '7.2', '8', '3.4', '21', '8.9', '4', '1.7', '11', '4.7', '3', '1.3', '4', '1.7', '4', '1.7', '0', '0.0', '49.2', ' ( X ) ', '100', '42.4', '99', '41.9', '96', '40.7', '24', '10.2', '22', '9.3', '118', '50.0', '4', '1.7', '2', '0.8', '8', '3.4', '7', '3.0', '3', '1.3', '4', '1.7', '5', '2.1', '6', '2.5', '11', '4.7', '10', '4.2', '14', '5.9', '8', '3.4', '15', '6.4', '9', '3.8', '3', '1.3', '2', '0.8', '4', '1.7', '3', '1.3', '49.7', ' ( X ) ', '100', '42.4', '97', '41.1', '97', '41.1', '31', '13.1', '21', '8.9', '236', '100.0', '232', '98.3', '227', '96.2', '2', '0.8', '0', '0.0', '2', '0.8', '0', '0.0', '0', '0.0', '1', '0.4', '0', '0.0', '0', '0.0', '0', '0.0', '1', '0.4', '0', '0.0', '0', '0.0', '0', '0.0', '0', '0.0', '0', '0.0', '1', '0.4', '4', '1.7', '1', '0.4', '0', '0.0', '0', '0.0', '3', '1.3', '231', '97.9', '2', '0.8', '1', '0.4', '2', '0.8', '0', '0.0', '4', '1.7', '236', '100.0', '2', '0.8', '1', '0.4', '0', '0.0', '0', '0.0', '1', '0.4', '234', '99.2', '236', '100.0', '2', '0.8', '2', '0.8', '0', '0.0', '0', '0.0', '0', '0.0', '0', '0.0', '0', '0.0', '0', '0.0', '234', '99.2', '225', '95.3', '2', '0.8', '0', '0.0', '2', '0.8', '0', '0.0', '1', '0.4', '4', '1.7', '236', '100.0', '236', '100.0', '120', '50.8', '60', '25.4', '42', '17.8', '37', '15.7', '6', '2.5', '3', '1.3', '0', '0.0', '8', '3.4', '0', '0.0', '3', '1.3', '7', '3.0', '0', '0.0', '0', '0.0', '0', '0.0', '0', '0.0', '0', '0.0', '0', '0.0', '0', '0.0', '120', '100.0', '65', '54.2', '20', '16.7', '60', '50.0', '19', '15.8', '2', '1.7', '0', '0.0', '3', '2.5', '1', '0.8', '55', '45.8', '47', '39.2', '27', '22.5', '4', '3.3', '20', '16.7', '8', '6.7', '23', '19.2', '34', '28.3', '1.97', ' ( X ) ', '2.66', ' ( X ) ', '660', '100.0', '120', '18.2', '540', '81.8', '4', '0.6', '1', '0.2', '4', '0.6', '1', '0.2', '527', '79.8', '3', '0.5', '6.3', ' ( X ) ', '6.0', ' ( X ) ', '120', '100.0', '58', '48.3', '116', ' ( X ) ', '2.00', ' ( X ) ', '62', '51.7', '120', ' ( X ) ', '1.94', ' ( X ) ']\n"
     ]
    }
   ],
   "source": [
    "print(census_struc[2]) # let's explore more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4) Moving from Notebooks to \\*.py Files** \n",
    "\n",
    "But before you go to do that, we need to start moving the findings of our exploration into a dedicated python file for cleaning the Census data. \n",
    "\n",
    "We're making this migration because notebooks are great for exploring data, but as our files and project grow larger, it is simpler to run the Python files outside notebooks AND sometimes very large files can cause our notebooks to crash.\n",
    "\n",
    "Here's what that new Python file needs to do:\n",
    "\n",
    "1. read the census data file into a variable\n",
    "2. clean the data by removing the \"...,...\" problem using a UDF\n",
    "3. create a nested data structure\n",
    "4. remove unwanted columns\n",
    "5. create a new string from the nested structure\n",
    "6. write the file to disk\n",
    "\n",
    "We've already #1, #2, #3, and #6 together, so we'll isolate those below along with comments to do the other work before putting in its own file.\n",
    "\n",
    "And recall that our UDFs has to be put ahead of the main program to work because the Main Program needs to know what happens in those UDFs.\n",
    "\n",
    "Below is the exact code that will be put into its own file named `clean_census.py` and we'll run it together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "#@@@@@  UDF unquotable() @@@@@@@@@@@@@@@\n",
    "# removes pesky , within \"\" values before changing , to \\t in program\n",
    "\n",
    "def unquotable(input_string):\n",
    "    \n",
    "    # remove \" char by splitting the input string into a list using that char\n",
    "    tmp_list = input_string.split('\"')\n",
    " \n",
    "    # remove , char\n",
    "    for i in range(len(tmp_list)):\n",
    "        # skip items that start/end with commas as these aren't quoted items\n",
    "        if (tmp_list[i][0] == \",\") or (tmp_list[i][-1] == \",\"):\n",
    "            continue\n",
    "        # replace , with - in quoted items\n",
    "        else:\n",
    "            tmp_list[i] = tmp_list[i].replace(\",\",\"-\")\n",
    "\n",
    "    # rejoin items into a string\n",
    "    output = \",\".join(tmp_list)\n",
    "\n",
    "    # get rid of any \",,\" and \",,,\" as it will add extra columns\n",
    "    output = output.replace(\",,,\",\",\")\n",
    "    output = output.replace(\",,\",\",\")\n",
    "    \n",
    "    # return string\n",
    "    return output\n",
    "\n",
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@    \n",
    "#@@@@@@@@ UDF nester() @@@@@@@@@\n",
    "# take a dataset string where each row is separated by input_row_delim \n",
    "# and each column is separate by input_col_delim to create a nested object of lists\n",
    "\n",
    "def nester(input_string,input_row_delim,input_col_delim):\n",
    "\n",
    "    # create a list item for each row in the file using the row delimiter\n",
    "    row_list = input_string.split(input_row_delim)\n",
    "\n",
    "    #output var\n",
    "    nested_data = []\n",
    "    \n",
    "    # created nested structure to store each column separately\n",
    "    # list of rows where each row is a list of columns)\n",
    "    for i in range(len(row_list)): \n",
    "        row = row_list[i]\n",
    "        col = row.split(input_col_delim)\n",
    "        nested_data.append(col)\n",
    "    \n",
    "    # return the nested structure\n",
    "    return nested_data\n",
    "\n",
    "\n",
    "################################################################\n",
    "##### MAIN PROGRAM #############################################\n",
    "################################################################\n",
    "\n",
    "# 1. read the census data file into a variable\n",
    "\n",
    "file_handle = open('C:\\\\Users\\\\colling\\\\!dwd_spring2019\\\\classes\\\\class7\\\\raw_census_2010.csv',\"r\")\n",
    "census_data = file_handle.read()\n",
    "file_handle.close()\n",
    "\n",
    "# 2. clean the data by removing the \"...,...\" problem using a UDF\n",
    "\n",
    "clean_census_data = unquotable(census_data)\n",
    "\n",
    "# 3. create a nested data structure with a UDF\n",
    "\n",
    "nested_census_data = nester(clean_census_data,\"\\n\",\",\")\n",
    "\n",
    "# 4. remove unwanted columns using a UDF\n",
    "\n",
    "# HOMEWORK\n",
    "\n",
    "# 5. create a new string from the nested structure using a UDF\n",
    "\n",
    "# HOMEWORK\n",
    "\n",
    "# 6. write the file to disk\n",
    "\n",
    "file_handle = open('C:\\\\Users\\\\colling\\\\!dwd_spring2019\\\\classes\\\\class7\\\\clean_census_2010.csv',\"w\")\n",
    "file_handle.write('final data var from step #5 goes here')\n",
    "file_handle.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
