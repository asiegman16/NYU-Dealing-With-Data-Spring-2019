{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files and Printing\n",
    "------------------\n",
    "You'll often be reading data from a file, or writing the output of your python scripts back into a file. Python makes this very easy. You need to open a file in the appropriate mode, using the `open` function, then you can read or write to accomplish your task. The `open` function takes two arguments, the name of the file, and the mode. The mode is a single letter string that specifies if you're going to be reading from a file, writing to a file, or appending to the end of an existing file. The function returns a file object that performs the various tasks you'll be performing: `a_file = open(filename, mode)`. The modes are:\n",
    "\n",
    "+ `'r'`: open a file for reading\n",
    "+ `'w'`: open a file for writing. Caution: this will overwrite any previously existing file\n",
    "+ `'a'`: append. Write to the end of a file. \n",
    "\n",
    "When reading, you typically want to iterate through the lines in a file using a for loop, as above. Some other common methods for dealing with files are: \n",
    "\n",
    "+ `file.read()`: read the entire contents of a file into a string\n",
    "+ `file.write(some_string)`: writes to the file, note this doesn't automatically include any new lines. Also note that sometimes writes are buffered- python will wait until you have several writes pending, and perform them all at once\n",
    "+ `file.flush()`: write out any buffered writes\n",
    "+ `file.close()`: close the open file. This will free up some computer resources occupied by keeping a file open.\n",
    "\n",
    "Here is an example using files:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing a file to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the file temp.txt, and get it ready for writing\n",
    "f = open(\"temp.txt\", \"w\")\n",
    "f.write(\"This is my first file! The end!\\n\")\n",
    "f.write(\"Oh wait, I wanted to say something else.\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check that we did everything as expected\n",
    "# the command below is one of the IPython \"magics\" - commands within the notebook unrelated to python\n",
    "# %magic shows you the list of basic commands and %lsmagic shows you all the super commands\n",
    "%more temp.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading a file from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now open the file for reading\n",
    "f = open(\"temp.txt\", \"r\")\n",
    "# And we read the full content of the file in memory, as a big string\n",
    "content = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we read the file, we have the lines in a big string. Let's process that big string a little bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file in the cell above, the content is in f2_content\n",
    "\n",
    "# Split the content of the file using the newline character \\n\n",
    "lines = content.split(\"\\n\")\n",
    "\n",
    "# Iterate through the line variable (it is a list of strings)\n",
    "# and then print the length of each line\n",
    "for line in lines:\n",
    "    print(line, \" ===> \", len(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a file numbers.txt and write the numbers from 0 to 24 there\n",
    "f = open(\"numbers.txt\", \"w\")\n",
    "for num in range(25):\n",
    "    f.write(str(num)+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check that we did everything as expected\n",
    "%more numbers.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now open the file for reading\n",
    "f = open(\"numbers.txt\", \"r\")\n",
    "# And we read the full content of the file in memory, as a big string\n",
    "content = f.read()\n",
    "f.close()\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we convert the strings into integers\n",
    "# we have the conditional to avoid trying to parse the string '' that \n",
    "# is at the end of the list\n",
    "numbers = []\n",
    "lines = content.split(\"\\n\")\n",
    "for line in lines:\n",
    "    if len(line) > 0:\n",
    "        numbers.append(int(line))\n",
    "    else:\n",
    "        continue\n",
    "print(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's clean up\n",
    "# windows\n",
    "#!del temp.txt\n",
    "#!del numbers.txt\n",
    "\n",
    "# macOS\n",
    "#!rm temp.txt\n",
    "#!rm numbers.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python `os` standard library\n",
    "Another addition to our file handling toolkit is the `os` library which provides ways to move files, make directories, and gather data about the file system. Like other standard libraries, we need to import it to use it via `import os`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# let's get information about our current working directory - the folder our Python applications are working within\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next, let's list everything in the directory\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... and see what our login name is\n",
    "os.getlogin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... and change our directory.\n",
    "os.chdir(\"sql-class\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... and change it back\n",
    "os.chdir(\"..\")  # \"..\" is a relative directory shortcut that means \"my parent folder\" no matter where you are\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .. and find out the \"separate\" used in constructing file paths\n",
    "# every operating system is different, and this value enables your python\n",
    "# to be cross-platform \n",
    "os.path.sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... now we can create our own paths for new files - important for creating a \"clean\" version of source data\n",
    "dir_list = os.getcwd().split(os.path.sep)\n",
    "print(dir_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... now let's create an output file in a new sub-folder called 7-tmp\n",
    "\n",
    "# first - append the new folder name \n",
    "# and create a file path string using os.path.sep and the string.join() method\n",
    "dir_list.append(\"7-tmp\")\n",
    "dir_string = os.path.sep.join(dir_list)\n",
    "\n",
    "# second - create the directory using the file path\n",
    "os.mkdir(dir_string)\n",
    "\n",
    "# third - add the file name \"tmp2.txt\" to the path\n",
    "dir_list.append(\"tmp2.txt\")\n",
    "dir_string = os.path.sep.join(dir_list)\n",
    "print(dir_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now open the file for writing using the absolute path \n",
    "# note: we could have also used os.chdir(\"7-tmp\") and open(\"tmp2.txt\")\n",
    "# but it's better programming to nearly always use absolute paths\n",
    "file_handle = open(dir_string,\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_handle.write(\"test file\\nsecond line\\n\")\n",
    "file_handle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Putting our Python to work:** \n",
    "# **Exploring and Cleaning the Census data file**\n",
    "\n",
    "So far in our class project, we have identified three data files (IRS tax return counts for NYC, US census data for NYC, and NYC film permits data) and we have skipped over the cleaning step to transform them in SQL.\n",
    "\n",
    "Now, we'll take one step back and begin cleaning them, starting with the smallest file - the US Census data.\n",
    "\n",
    "Our approach will be as follows:\n",
    "1. Inspect the data thoroughly to understand its benefits and risks for processing, and what information lies within\n",
    "* Clean/fix the data to remove any issues that would prevent it from working with SQL, such as weird characters, too many columns, missing data, splitting values into multiple rows, or combining multiple values into a single row\n",
    "* Structure the data found in the file as a Python native structure so we can manipulate and prepare it for use in SQL\n",
    "* Migrate our approach to a dedicated Python file\n",
    "\n",
    "We'll use: file read/write, loops, nested structures and UDFs to do all of this. (Some will be homework.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1- INSPECT DATA** \n",
    "First - we need to understand the data in this file - using either Excel or JupyterLab's CSV reader.\n",
    "\n",
    "Upon inspection that way, here's what we learn:\n",
    "* there's 350+ columns\n",
    "* there's zipcodes for just NYC \n",
    "* there's a mix of letter and numbers for values\n",
    "* we have both percent and numbers values\n",
    "* it appears to be comma-separated\n",
    "* there are two column header lines: one with codes, and another with human-readable labels\n",
    "\n",
    "Next - we inspect with Python in two ways: first we'll look at the raw file as a string, then we'll probe whether the comma-separation is fail safe or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **REVIEW DATA AS STRING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\colling\\\\!dwd_spring2019\\\\classes\\\\class7\\\\IPYNB solved'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we'll use our newly learned ability to read files to capture the Census file as a string\n",
    "# to use this command, we'll need the full file path for our census file\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the location of the raw_census_2010.csv file you downloaded and open it for reading into a variable\n",
    "file_handle = open('C:\\\\Users\\\\colling\\\\!dwd_spring2019\\\\classes\\\\class7\\\\raw_census_2010.csv',\"r\")\n",
    "census_data = file_handle.read()\n",
    "file_handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GEO.id,GEO.id2,GEO.display-label,HD01_S001,HD02_S001,HD01_S002,HD02_S002,HD01_S003,HD02_S003,HD01_S004,HD02_S004,HD01_S005,HD02_S005,HD01_S006,HD02_S006,HD01_S007,HD02_S007,HD01_S008,HD02_S008,HD01_S009,HD02_S009,HD01_S010,HD02_S010,HD01_S011,HD02_S011,HD01_S012,HD02_S012,HD01_S013,HD02_S013,HD01_S014,HD02_S014,HD01_S015,HD02_S015,HD01_S016,HD02_S016,HD01_S017,HD02_S017,HD01_S018,HD02_S018,HD01_S019,HD02_S019,HD01_S020,HD02_S020,HD01_S021,HD02_S021,HD01_S022,HD02_S022,HD01_S023,HD02_S023,HD01_S024,HD02_S024,HD01_S025,HD02_S025,HD01_S026,HD02_S026,HD01_S027,HD02_S027,HD01_S028,HD02_S028,HD01_S029,HD02_S029,HD01_S030,HD02_S030,HD01_S031,HD02_S031,HD01_S032,HD02_S032,HD01_S033,HD02_S033,HD01_S034,HD02_S034,HD01_S035,HD02_S035,HD01_S036,HD02_S036,HD01_S037,HD02_S037,HD01_S038,HD02_S038,HD01_S039,HD02_S039,HD01_S040,HD02_S040,HD01_S041,HD02_S041,HD01_S042,HD02_S042,HD01_S043,HD02_S043,HD01_S044,HD02_S044,HD01_S045,HD02_S045,HD01_S046,HD02_S046,HD01_S047,HD02_S047,HD01_S048,HD02_S048,HD01_S049,HD02_S049,HD01_S050,HD02_S050,HD01_S051,HD02_S051,HD01_S052,HD02_S052,HD01_S053,HD02_S053,HD01_S054,HD02_S054,HD01_S055,HD02_S055,HD01_S056,HD02_S056,HD01_S057,HD02_S057,HD01_S058,HD02_S058,HD01_S059,HD02_S059,HD01_S060,HD02_S060,HD01_S061,HD02_S061,HD01_S062,HD02_S062,HD01_S063,HD02_S063,HD01_S064,HD02_S064,HD01_S065,HD02_S065,HD01_S066,HD02_S066,HD01_S067,HD02_S067,HD01_S068,HD02_S068,HD01_S069,HD02_S069,HD01_S070,HD02_S070,HD01_S071,HD02_S071,HD01_S072,HD02_S072,HD01_S073,HD02_S073,HD01_S074,HD02_S074,HD01_S075,HD02_S075,HD01_S076,HD02_S076,HD01_S077,HD02_S077,HD01_S078,HD02_S078,HD01_S079,HD02_S079,HD01_S080,HD02_S080,HD01_S081,HD02_S081,HD01_S082,HD02_S082,HD01_S083,HD02_S083,HD01_S084,HD02_S084,HD01_S085,HD02_S085,HD01_S086,HD02_S086,HD01_S087,HD02_S087,HD01_S088,HD02_S088,HD01_S089,HD02_S089,HD01_S090,HD02_S090,HD01_S091,HD02_S091,HD01_S092,HD02_S092,HD01_S093,HD02_S093,HD01_S094,HD02_S094,HD01_S095,HD02_S095,HD01_S096,HD02_S096,HD01_S097,HD02_S097,HD01_S098,HD02_S098,HD01_S099,HD02_S099,HD01_S100,HD02_S100,HD01_S101,HD02_S101,HD01_S102,HD02_S102,HD01_S103,HD02_S103,HD01_S104,HD02_S104,HD01_S105,HD02_S105,HD01_S106,HD02_S106,HD01_S107,HD02_S107,HD01_S108,HD02_S108,HD01_S109,HD02_S109,HD01_S110,HD02_S110,HD01_S111,HD02_S111,HD01_S112,HD02_S112,HD01_S113,HD02_S113,HD01_S114,HD02_S114,HD01_S115,HD02_S115,HD01_S116,HD02_S116,HD01_S117,HD02_S117,HD01_S118,HD02_S118,HD01_S119,HD02_S119,HD01_S120,HD02_S120,HD01_S121,HD02_S121,HD01_S122,HD02_S122,HD01_S123,HD02_S123,HD01_S124,HD02_S124,HD01_S125,HD02_S125,HD01_S126,HD02_S126,HD01_S127,HD02_S127,HD01_S128,HD02_S128,HD01_S129,HD02_S129,HD01_S130,HD02_S130,HD01_S131,HD02_S131,HD01_S132,HD02_S132,HD01_S133,HD02_S133,HD01_S134,HD02_S134,HD01_S135,HD02_S135,HD01_S136,HD02_S136,HD01_S137,HD02_S137,HD01_S138,HD02_S138,HD01_S139,HD02_S139,HD01_S140,HD02_S140,HD01_S141,HD02_S141,HD01_S142,HD02_S142,HD01_S143,HD02_S143,HD01_S144,HD02_S144,HD01_S145,HD02_S145,HD01_S146,HD02_S146,HD01_S147,HD02_S147,HD01_S148,HD02_S148,HD01_S149,HD02_S149,HD01_S150,HD02_S150,HD01_S151,HD02_S151,HD01_S152,HD02_S152,HD01_S153,HD02_S153,HD01_S154,HD02_S154,HD01_S155,HD02_S155,HD01_S156,HD02_S156,HD01_S157,HD02_S157,HD01_S158,HD02_S158,HD01_S159,HD02_S159,HD01_S160,HD02_S160,HD01_S161,HD02_S161,HD01_S162,HD02_S162,HD01_S163,HD02_S163,HD01_S164,HD02_S164,HD01_S165,HD02_S165,HD01_S166,HD02_S166,HD01_S167,HD02_S167,HD01_S168,HD02_S168,HD01_S169,HD02_S169,HD01_S170,HD02_S170,HD01_S171,HD02_S171,HD01_S172,HD02_S172,HD01_S173,HD02_S173,HD01_S174,HD02_S174,HD01_S175,HD02_S175,HD01_S176,HD02_S176,HD01_S177,HD02_S177,HD01_S178,HD02_S178,HD01_S179,HD02_S179,HD01_S180,HD02_S180,HD01_S181,HD02_S181,HD01_S182,HD02_S182,HD01_S183,HD02_S183,HD01_S184,HD02_S184,HD01_S185,HD02_S185,HD01_S186,HD02_S186\\nId,Id2,Geography,Number; SEX AND AGE - Total population,Percent; SEX AND AGE - Total population,Number; SEX AND AGE - Total population - Under 5 years,Percent; SEX AND AGE - Total population - Under 5 years,Number; SEX AND AGE - Total population - 5 to 9 years,Percent; SEX AND AGE - Total population - 5 to 9 years,Number; SEX AND AGE - Total population - 10 to 14 years,Percent; SEX AND AGE - Total population - 10 to 14 years,Number; SEX AND AGE - Total population - 15 to 19 years,Percent; SEX AND AGE - Total population - 15 to 19 years,Number; SEX AND AGE - Total population - 20 to 24 years,Percent; SEX AND AGE - Total population - 20 to 24 years,Number; SEX AND AGE - Total population - 25 to 29 years,Percent; SEX AND AGE - Total population - 25 to 29 years,Number; SEX AND AGE - Total population - 30 to 34 years,Percent; SEX AND AGE - Total population - 30 to 34 years,Number; SEX AND AGE - Total population - 35 to 39 years,Percent; SEX AND AGE - Total population - 35 to 39 years,Number; SEX AND AGE - Total population - 40 to 44 years,Percent; SEX AND AGE - Total population - 40 to 44 years,Number; SEX AND AGE - Total population - 45 to 49 years,Percent; SEX AND AGE - Total population - 45 to 49 years,Number; SEX AND AGE - Total'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the first 5000 characters of the census file\n",
    "census_data[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "',812,12.3,400,6.0,9,0.1,86,1.3,16,0.2,34,0.5,267,4.0,3.0, ( X ) ,11.6, ( X ) ,5805,100.0,2766,47.6,6571, ( X ) ,2.38, ( X ) ,3039,52.4,6819, ( X ) ,2.24, ( X ) \\n8600000US14903,14903,ZCTA5 14903,7567,100.0,439,5.8,491,6.5,494,6.5,484,6.4,426,5.6,448,5.9,425,5.6,414,5.5,532,7.0,613,8.1,643,8.5,557,7.4,451,6.0,298,3.9,257,3.4,217,2.9,203,2.7,175,2.3,41.6, ( X ) ,6049,79.9,5837,77.1,5566,73.6,1404,18.6,1150,15.2,3690,48.8,220,2.9,253,3.3,249,3.3,250,3.3,203,2.7,212,2.8,225,3.0,191,2.5,255,3.4,304,4.0,335,4.4,272,3.6,229,3.0,137,1.8,122,1.6,99,1.3,80,1.1,54,0.7,40.9, ( X ) ,2923,38.6,2810,37.1,2675,35.4,626,8.3,492,6.5,3877,51.2,219,2.9,238,3.1,245,3.2,234,3.1,223,2.9,236,3.1,200,2.6,223,2.9,277,3.7,309,4.1,308,4.1,285,3.8,222,2.9,161,2.1,135,1.8,118,1.6,123,1.6,121,1.6,42.1, ( X ) ,3126,41.3,3027,40.0,2891,38.2,778,10.3,658,8.7,7567,100.0,7434,98.2,7159,94.6,129,1.7,10,0.1,101,1.3,19,0.3,35,0.5,5,0.1,7,0.1,8,0.1,2,0.0,25,0.3,2,0.0,0,0.0,2,0.0,0,0.0,0,0.0,33,0.4,133,1.8,17,0.2,24,0.3,74,1.0,3,0.0,7283,96.2,214,2.8,36,0.5,127,1.7,4,0.1,40,0.5,7567,100.0,102,1.3,22,0.3,42,0.6,7,0.1,31,0.4,7465,98.7,7567,100.0,102,1.3,62,0.8,5,0.1,0,0.0,0,0.0,0,0.0,30,0.4,5,0.1,7465,98.7,7097,93.8,124,1.6,10,0.1,101,1.3,2,0.0,3,0.0,128,1.7,7567,100.0,7557,99.9,3231,42.7,1451,19.2,2125,28.1,1560,20.6,331,4.4,131,1.7,61,0.8,419,5.5,34,0.4,21,0.3,296,3.9,10,0.1,10,0.1,6,0.1,4,0.1,0,0.0,0,0.0,0,0.0,3231,100.0,2040,63.1,880,27.2,1451,44.9,538,16.7,163,5.0,95,2.9,426,13.2,247,7.6,1191,36.9,1011,31.3,472,14.6,122,3.8,539,16.7,279,8.6,966,29.9,870,26.9,2.34, ( X ) ,2.92, ( X ) ,3440,100.0,3231,93.9,209,6.1,45,1.3,6,0.2,38,1.1,30,0.9,18,0.5,72,2.1,1.8, ( X ) ,3.8, ( X ) ,3231,100.0,2095,64.8,5325, ( X ) ,2.54, ( X ) ,1136,35.2,2232, ( X ) ,1.96, ( X ) \\n8600000US14904,14904,ZCTA5 14904,16269,100.0,1186,7.3,1159,7.1,1104,6.8,1111,6.8,1076,6.6,1159,7.1,1004,6.2,880,5.4,936,5.8,1250,7.7,1265,7.8,1049,6.4,863,5.3,574,3.5,479,2.9,434,2.7,368,2.3,372,2.3,36.8, ( X ) ,12595,77.4,12126,74.5,11473,70.5,2723,16.7,2227,13.7,7826,48.1,601,3.7,595,3.7,582,3.6,556,3.4,537,3.3,532,3.3,497,3.1,442,2.7,453,2.8,610,3.7,615,3.8,492,3.0,402,2.5,259,1.6,235,1.4,164,1.0,132,0.8,122,0.7,35.1, ( X ) ,5939,36.5,5699,35.0,5373,33.0,1133,7.0,912,5.6,8443,51.9,585,3.6,564,3.5,522,3.2,555,3.4,539,3.3,627,3.9,507,3.1,438,2.7,483,3.0,640,3.9,650,4.0,557,3.4,461,2.8,315,1.9,244,1.5,270,1.7,236,1.5,250,1.5,38.8, ( X ) ,6656,40.9,6427,39.5,6100,37.5,1590,9.8,1315,8.1,16269,100.0,15636,96.1,14402,88.5,1038,6.4,54,0.3,66,0.4,3,0.0,15,0.1,20,0.1,3,0.0,9,0.1,5,0.0,11,0.1,7,0.0,6,0.0,0,0.0,1,0.0,0,0.0,69,0.4,633,3.9,108,0.7,26,0.2,378,2.3,29,0.2,15003,92.2,1490,9.2,216,1.3,99,0.6,26,0.2,129,0.8,16269,100.0,473,2.9,77,0.5,284,1.7,12,0.1,100,0.6,15796,97.1,16269,100.0,473,2.9,254,1.6,66,0.4,1,0.0,1,0.0,5,0.0,58,0.4,88,0.5,15796,97.1,14148,87.0,972,6.0,53,0.3,65,0.4,2,0.0,11,0.1,545,3.3,16269,100.0,16190,99.5,6776,41.6,2499,15.4,4828,29.7,3562,21.9,826,5.1,410,2.5,96,0.6,1261,7.8,157,1.0,47,0.3,738,4.5,79,0.5,8,0.0,7,0.0,1,0.0,71,0.4,37,0.2,34,0.2,6776,100.0,4111,60.7,1898,28.0,2499,36.9,884,13.0,420,6.2,245,3.6,1192,17.6,769,11.3,2665,39.3,2193,32.4,933,13.8,227,3.4,1260,18.6,607,9.0,2148,31.7,1742,25.7,2.39, ( X ) ,2.98, ( X ) ,7324,100.0,6776,92.5,548,7.5,209,2.9,9,0.1,49,0.7,23,0.3,24,0.3,234,3.2,1.2, ( X ) ,7.1, ( X ) ,6776,100.0,4055,59.8,9873, ( X ) ,2.43, ( X ) ,2721,40.2,6317, ( X ) ,2.32, ( X ) \\n8600000US14905,14905,ZCTA5 14905,9070,100.0,591,6.5,574,6.3,540,6.0,516,5.7,416,4.6,538,5.9,527,5.8,498,5.5,598,6.6,609,6.7,705,7.8,722,8.0,630,6.9,445,4.9,317,3.5,267,2.9,268,3.0,309,3.4,42.8, ( X ) ,7247,79.9,7029,77.5,6776,74.7,1987,21.9,1606,17.7,4331,47.8,308,3.4,304,3.4,280,3.1,271,3.0,209,2.3,247,2.7,255,2.8,235,2.6,296,3.3,295,3.3,341,3.8,332,3.7,294,3.2,202,2.2,142,1.6,121,1.3,103,1.1,96,1.1,40.9, ( X ) ,3378,37.2,3262,36.0,3137,34.6,848,9.3,664,7.3,4739,52.2,283,3.1,270,3.0,260,2.9,245,2.7,207,2.3,291,3.2,272,3.0,263,2.9,302,3.3,314,3.5,364,4.0,390,4.3,336,3.7,243,2.7,175,1.9,146,1.6,165,1.8,213,2.3,44.6, ( X ) ,3869,42.7,3767,41.5,3639,40.1,1139,12.6,942,10.4,9070,100.0,8809,97.1,8100,89.3,534,5.9,13,0.1,104,1.1,40,0.4,16,0.2,10,0.1,3,0.0,8,0.1,5,0.1,22,0.2,4,0.0,0,0.0,0,0.0,4,0.0,0,0.0,54,0.6,261,2.9,25,0.3,22,0.2,170,1.9,14,0.2,8353,92.1,729,8.0,60,0.7,131,1.4,5,0.1,74,0.8,9070,100.0,166,1.8,27,0.3,84,0.9,3,0.0,52,0.6,8904,98.2,9070,100.0,166,1.8,80,0.9,14,0.2,2,0.0,0,0.0,0,0.0,38,0.4,32,0.4,8904,98.2,8020,88.4,520,5.7,11,0.1,104,1.1,4,0.0,16,0.2,229,2.5,9070,100.0,8988,99.1,4024,44.4,1802,19.9,2373,26.2,1878,20.7,281,3.1,104,1.1,59,0.7,508,5.6,49,0.5,33,0.4,297,3.3,82,0.9,47,0.5,19,0.2,28,0.3,35,0.4,10,0.1,25,0.3,4024,100.0,2434,60.5,1035,25.7,1802,44.8,647,16.1,172,4.3,98,2.4,460,11.4,290,7.2,1590,39.5,1331,33.1,507,12.6,131,3.3,824,20.5,403,10.0,1120,27.8,1169,29.1,2.23, ( X ) ,2.83, ( X ) ,4326,100.0,4024,93.0,302,7.0,75,1.7,5,0.1,49,1.1,24,0.6,32,0.7,117,2.7,1.8, ( X ) ,5.4, ( X ) ,4024,100.0,2726,67.7,6370, ( X ) ,2.34, ( X ) ,1298,32.3,2618, ( X ) ,2.02, ( X ) \\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and the last 5000 characters\n",
    "census_data[-5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **REVIEW COMMA-based SEPARABILITY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GEO.id,GEO.id2,GEO.display-label,HD01_S001,HD02_S001,HD01_S002,HD02_S002,HD01_S003,HD02_S003,HD01_S004,HD02_S004,HD01_S005,HD02_S005,HD01_S006,HD02_S006,HD01_S007,HD02_S007,HD01_S008,HD02_S008,HD01_S009,HD02_S009,HD01_S010,HD02_S010,HD01_S011,HD02_S011,HD01_S012,HD02_S012,HD01_S013,HD02_S013,HD01_S014,HD02_S014,HD01_S015,HD02_S015,HD01_S016,HD02_S016,HD01_S017,HD02_S017,HD01_S018,HD02_S018,HD01_S019,HD02_S019,HD01_S020,HD02_S020,HD01_S021,HD02_S021,HD01_S022,HD02_S022,HD01_S023,HD02_S023,HD01_S024,HD02_S024,HD01_S025,HD02_S025,HD01_S026,HD02_S026,HD01_S027,HD02_S027,HD01_S028,HD02_S028,HD01_S029,HD02_S029,HD01_S030,HD02_S030,HD01_S031,HD02_S031,HD01_S032,HD02_S032,HD01_S033,HD02_S033,HD01_S034,HD02_S034,HD01_S035,HD02_S035,HD01_S036,HD02_S036,HD01_S037,HD02_S037,HD01_S038,HD02_S038,HD01_S039,HD02_S039,HD01_S040,HD02_S040,HD01_S041,HD02_S041,HD01_S042,HD02_S042,HD01_S043,HD02_S043,HD01_S044,HD02_S044,HD01_S045,HD02_S045,HD01_S046,HD02_S046,HD01_S047,HD02_S047,HD01_S048,HD02_S048,HD01_S049,HD02_S049,HD01_S050,HD02_S050,HD01_S051,HD02_S051,HD01_S052,HD02_S052,HD01_S053,HD02_S053,HD01_S054,HD02_S054,HD01_S055,HD02_S055,HD01_S056,HD02_S056,HD01_S057,HD02_S057,HD01_S058,HD02_S058,HD01_S059,HD02_S059,HD01_S060,HD02_S060,HD01_S061,HD02_S061,HD01_S062,HD02_S062,HD01_S063,HD02_S063,HD01_S064,HD02_S064,HD01_S065,HD02_S065,HD01_S066,HD02_S066,HD01_S067,HD02_S067,HD01_S068,HD02_S068,HD01_S069,HD02_S069,HD01_S070,HD02_S070,HD01_S071,HD02_S071,HD01_S072,HD02_S072,HD01_S073,HD02_S073,HD01_S074,HD02_S074,HD01_S075,HD02_S075,HD01_S076,HD02_S076,HD01_S077,HD02_S077,HD01_S078,HD02_S078,HD01_S079,HD02_S079,HD01_S080,HD02_S080,HD01_S081,HD02_S081,HD01_S082,HD02_S082,HD01_S083,HD02_S083,HD01_S084,HD02_S084,HD01_S085,HD02_S085,HD01_S086,HD02_S086,HD01_S087,HD02_S087,HD01_S088,HD02_S088,HD01_S089,HD02_S089,HD01_S090,HD02_S090,HD01_S091,HD02_S091,HD01_S092,HD02_S092,HD01_S093,HD02_S093,HD01_S094,HD02_S094,HD01_S095,HD02_S095,HD01_S096,HD02_S096,HD01_S097,HD02_S097,HD01_S098,HD02_S098,HD01_S099,HD02_S099,HD01_S100,HD02_S100,HD01_S101,HD02_S101,HD01_S102,HD02_S102,HD01_S103,HD02_S103,HD01_S104,HD02_S104,HD01_S105,HD02_S105,HD01_S106,HD02_S106,HD01_S107,HD02_S107,HD01_S108,HD02_S108,HD01_S109,HD02_S109,HD01_S110,HD02_S110,HD01_S111,HD02_S111,HD01_S112,HD02_S112,HD01_S113,HD02_S113,HD01_S114,HD02_S114,HD01_S115,HD02_S115,HD01_S116,HD02_S116,HD01_S117,HD02_S117,HD01_S118,HD02_S118,HD01_S119,HD02_S119,HD01_S120,HD02_S120,HD01_S121,HD02_S121,HD01_S122,HD02_S122,HD01_S123,HD02_S123,HD01_S124,HD02_S124,HD01_S125,HD02_S125,HD01_S126,HD02_S126,HD01_S127,HD02_S127,HD01_S128,HD02_S128,HD01_S129,HD02_S129,HD01_S130,HD02_S130,HD01_S131,HD02_S131,HD01_S132,HD02_S132,HD01_S133,HD02_S133,HD01_S134,HD02_S134,HD01_S135,HD02_S135,HD01_S136,HD02_S136,HD01_S137,HD02_S137,HD01_S138,HD02_S138,HD01_S139,HD02_S139,HD01_S140,HD02_S140,HD01_S141,HD02_S141,HD01_S142,HD02_S142,HD01_S143,HD02_S143,HD01_S144,HD02_S144,HD01_S145,HD02_S145,HD01_S146,HD02_S146,HD01_S147,HD02_S147,HD01_S148,HD02_S148,HD01_S149,HD02_S149,HD01_S150,HD02_S150,HD01_S151,HD02_S151,HD01_S152,HD02_S152,HD01_S153,HD02_S153,HD01_S154,HD02_S154,HD01_S155,HD02_S155,HD01_S156,HD02_S156,HD01_S157,HD02_S157,HD01_S158,HD02_S158,HD01_S159,HD02_S159,HD01_S160,HD02_S160,HD01_S161,HD02_S161,HD01_S162,HD02_S162,HD01_S163,HD02_S163,HD01_S164,HD02_S164,HD01_S165,HD02_S165,HD01_S166,HD02_S166,HD01_S167,HD02_S167,HD01_S168,HD02_S168,HD01_S169,HD02_S169,HD01_S170,HD02_S170,HD01_S171,HD02_S171,HD01_S172,HD02_S172,HD01_S173,HD02_S173,HD01_S174,HD02_S174,HD01_S175,HD02_S175,HD01_S176,HD02_S176,HD01_S177,HD02_S177,HD01_S178,HD02_S178,HD01_S179,HD02_S179,HD01_S180,HD02_S180,HD01_S181,HD02_S181,HD01_S182,HD02_S182,HD01_S183,HD02_S183,HD01_S184,HD02_S184,HD01_S185,HD02_S185,HD01_S186,HD02_S186\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in the first 5000 characters, we see that the census has \"code labels\" for each column\n",
    "# none of which have funky characters that could trip us up with comma separation\n",
    "# we need to work with just that first line - we'll use readline on the file handle\n",
    "file_handle = open('C:\\\\Users\\\\colling\\\\!dwd_spring2019\\\\classes\\\\class7\\\\raw_census_2010.csv',\"r\")\n",
    "census_first_line = file_handle.readline()\n",
    "file_handle.close()\n",
    "census_first_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's separate that first line into a list and see exactly how many columns we have\n",
    "header1_list = census_first_line.split(\",\")\n",
    "len(header1_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We knew it had 350+ and this is too many columns to effectively work with. We know from our transform work together that we need to weight up/down the data, which means percentages won't be useful, so we can remove those, and we can also remove some of the family occupancy data because it was decided to focus on gender, income and ethnicity in the project.\n",
    "\n",
    "Before we get to removing data, we need to put our data into a structure we can slice and dice. In other words, we need to transform our \"string\" data into a list of lists - a nested data structure where each row is a list, and within that list, each column is a list - a nested data structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example: let's look at this simple 3x3 table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c']\n",
      "[123, 456, 789]\n",
      "[987, 654, 321]\n",
      "row 1, column 3 ==> c\n",
      "row 2, column 2 ==> 456\n",
      "row 3, column 1 ==> 987\n"
     ]
    }
   ],
   "source": [
    "example_list = [ \n",
    "[\"a\",\"b\",\"c\"],\n",
    "[123,456,789],\n",
    "[987,654,321]\n",
    "]\n",
    "print(example_list[0])\n",
    "print(example_list[1])\n",
    "print(example_list[2])\n",
    "print(\"row 1, column 3 ==>\",example_list[0][2])\n",
    "print(\"row 2, column 2 ==>\",example_list[1][1])\n",
    "print(\"row 3, column 1 ==>\",example_list[2][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2- CLEANING UP WONKY DATA**\n",
    "\n",
    "But before we can even create a nested structure - we need to be confident we can split the data correctly for every single line.\n",
    "\n",
    "Unfortunately, CSV data is known to be particularly  can be tricky because sometimes data sources use commas in column labels but surround those column headers with \"\" because Excel will treat it right. \n",
    "\n",
    "Python won't be so forgiving so we need to test for \"\" in the data - we'll do this using a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "# use a for loop\n",
    "quote_cnt = 0\n",
    "for char in census_data:\n",
    "    if char == '\"':\n",
    "        quote_cnt += 1\n",
    "    else:\n",
    "        continue\n",
    "print(quote_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh-oh! those quotes spell trouble so now we need to see where that first quote appears and if a comma appears after it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 18 years,\"Number; HOUSEHOLDS BY TYPE - Total households - Family households (families) [7] - Male householder, no wife present\",\"'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quote_pos = census_data.find('\"')\n",
    "comma_pos = census_data.find(\",\",quote_pos)\n",
    "census_data[quote_pos - 10: comma_pos + 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We suspect that those \"\"  in human-readable column headers of the second line of data are hiding \"...**,**...\" and will cause any nesting using comma-separation to create extra columns.\n",
    "\n",
    "Let's prove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "391"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to isolate the second line \n",
    "# and we can do that using find: \n",
    "# the second line is between the first and second \\n\n",
    "first_nl = census_data.find(\"\\n\")\n",
    "second_nl = census_data.find(\"\\n\",first_nl+1)\n",
    "second_line = census_data[first_nl+1:second_nl]\n",
    "\n",
    "# split the second line using commas to see how many columns we get\n",
    "len(second_line.split(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've proven those will be a problem, we are going to use Python to clean those up via our own User Defined Function for this purpose, because it may appear in another data source, too.\n",
    "\n",
    "Before we create the UDF, we will describe what it will do\n",
    "1. it will accept a string input\n",
    "2. it will remove \" characters from the input\n",
    "3. when it finds a , between \"\" it will replace , characters in the input with a -. However, it will NOT change , otherwise since it is a CSV file.\n",
    "4. it will return a string output\n",
    "\n",
    "And, we will create a test_input and expected_output to test our UDF during development:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "#@@@@@  UDF unquotable() @@@@@@@@@@@@@@@\n",
    "# removes pesky , within \"\" values before changing , to \\t in program\n",
    "def unquotable(input_string):\n",
    "    \n",
    "    # remove \" char \n",
    "    # we remove it by splitting the input string into a list using that char\n",
    "    tmp_list = input_string.split('\"')\n",
    " \n",
    "    # remove , char\n",
    "    for i in range(len(tmp_list)):\n",
    "        # skip items that start/end with commas \n",
    "        # as these aren't quoted items \n",
    "        # and we don't want to remove their commas \n",
    "        if (tmp_list[i][0] == \",\") or (tmp_list[i][-1] == \",\"):\n",
    "            continue\n",
    "        # replace , with - in quoted items\n",
    "        else:\n",
    "            tmp_list[i] = tmp_list[i].replace(\",\",\"-\")\n",
    "\n",
    "    # rejoin items into a string\n",
    "    output = \",\".join(tmp_list)\n",
    "\n",
    "    # get rid of any \",,\" and \",,,\" that could exist as it will add extra columns\n",
    "    output = output.replace(\",,,\",\",\")\n",
    "    output = output.replace(\",,\",\",\")\n",
    "    \n",
    "    # return string\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our test data\n",
    "test_input = 'something,\"test string: a,b,c\",other thing'\n",
    "exp_output = 'something,test string: a-b-c,other thing'\n",
    "\n",
    "# our testing lines - run the UDF\n",
    "test_output = unquotable(test_input)\n",
    "\n",
    "# compare UDF run to expected output\n",
    "test_output == exp_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "375\n"
     ]
    }
   ],
   "source": [
    "# now we try our second_line variable storing the problem row for cleaning \n",
    "# proof it works: no \" and len after CSV split = 375\n",
    "new_row = unquotable(second_line)\n",
    "print('\"' in new_row)\n",
    "print(len(new_row.split(\",\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works! Now, let's fix our original input data string, `census_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_census_data = unquotable(census_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3-CREATING A NESTED STRUCTURE**\n",
    "Now that we've cleaned up the wonkiness in the data, we can create our nested structure using a `for` loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by reviewing our simple 3x3 table example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c']\n",
      "[123, 456, 789]\n",
      "[987, 654, 321]\n",
      "row 1, column 3 ==> c\n",
      "row 2, column 2 ==> 456\n",
      "row 3, column 1 ==> 987\n"
     ]
    }
   ],
   "source": [
    "example_list = [ \n",
    "[\"a\",\"b\",\"c\"],\n",
    "[123,456,789],\n",
    "[987,654,321]\n",
    "]\n",
    "print(example_list[0])\n",
    "print(example_list[1])\n",
    "print(example_list[2])\n",
    "print(\"row 1, column 3 ==>\",example_list[0][2])\n",
    "print(\"row 2, column 2 ==>\",example_list[1][1])\n",
    "print(\"row 3, column 1 ==>\",example_list[2][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example, the data is structured as:\n",
    "\n",
    "`list[row][column]`\n",
    "\n",
    "And we will use split commands to do the same for our data, this time creating another UDF.\n",
    "\n",
    "Again - as before - let's state what it will do and create test data:\n",
    "1. the UDF will take a data string as input\n",
    "* it will create a list where each line in the data, identified using `\\n`, is an item\n",
    "* for each item in the first list `list[row]`, it will create a list of columns, using comma-separatiuon (`,`) to identify each item\n",
    "* it would be nice for the UDF user to specify what character separates rows and columns, separately\n",
    "* the UDF will return the nested data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@    \n",
    "#@@@@@@@@ UDF nester() @@@@@@@@@\n",
    "# take a dataset string where each row is separated by input_row_delim \n",
    "# and each column is separate by input_col_delim to create a nested object of lists\n",
    "def nester(input_string,input_row_delim,input_col_delim):\n",
    "\n",
    "    # create a list item for each row in the file using the row delimiter\n",
    "    row_list = input_string.split(input_row_delim)\n",
    "\n",
    "    #output var\n",
    "    nested_data = []\n",
    "    \n",
    "    # created nested structure to store each column separately\n",
    "    # list of rows where each row is a list of columns)\n",
    "    for i in range(len(row_list)): \n",
    "        row = row_list[i]\n",
    "        col = row.split(input_col_delim)\n",
    "        nested_data.append(col)\n",
    "    \n",
    "    # return the nested structure\n",
    "    return nested_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our test data\n",
    "test_input = 'r1c1,r1c2,r1c3\\nr2c1,r2c2,r2c3\\nr3c1,r3c2,r3c3'\n",
    "exp_output = [\n",
    "    ['r1c1','r1c2','r1c3'],\n",
    "    ['r2c1','r2c2','r2c3'],\n",
    "    ['r3c1','r3c2','r3c3']\n",
    "]\n",
    "\n",
    "# our testing - run the UDF\n",
    "test_output = nester(test_input,'\\n',',')\n",
    "\n",
    "# compare UDF output to expected output\n",
    "test_output == exp_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a nested structure of our census data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# structure our original string\n",
    "census_struc = nester(clean_census_data,'\\n',',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8600000US06390', '06390', 'ZCTA5 06390', '236', '100.0', '6', '2.5', '9', '3.8', '16', '6.8', '11', '4.7', '7', '3.0', '7', '3.0', '7', '3.0', '13', '5.5', '20', '8.5', '27', '11.4', '22', '9.3', '29', '12.3', '19', '8.1', '20', '8.5', '6', '2.5', '6', '2.5', '8', '3.4', '3', '1.3', '49.4', ' ( X ) ', '200', '84.7', '196', '83.1', '193', '81.8', '55', '23.3', '43', '18.2', '118', '50.0', '2', '0.8', '7', '3.0', '8', '3.4', '4', '1.7', '4', '1.7', '3', '1.3', '2', '0.8', '7', '3.0', '9', '3.8', '17', '7.2', '8', '3.4', '21', '8.9', '4', '1.7', '11', '4.7', '3', '1.3', '4', '1.7', '4', '1.7', '0', '0.0', '49.2', ' ( X ) ', '100', '42.4', '99', '41.9', '96', '40.7', '24', '10.2', '22', '9.3', '118', '50.0', '4', '1.7', '2', '0.8', '8', '3.4', '7', '3.0', '3', '1.3', '4', '1.7', '5', '2.1', '6', '2.5', '11', '4.7', '10', '4.2', '14', '5.9', '8', '3.4', '15', '6.4', '9', '3.8', '3', '1.3', '2', '0.8', '4', '1.7', '3', '1.3', '49.7', ' ( X ) ', '100', '42.4', '97', '41.1', '97', '41.1', '31', '13.1', '21', '8.9', '236', '100.0', '232', '98.3', '227', '96.2', '2', '0.8', '0', '0.0', '2', '0.8', '0', '0.0', '0', '0.0', '1', '0.4', '0', '0.0', '0', '0.0', '0', '0.0', '1', '0.4', '0', '0.0', '0', '0.0', '0', '0.0', '0', '0.0', '0', '0.0', '1', '0.4', '4', '1.7', '1', '0.4', '0', '0.0', '0', '0.0', '3', '1.3', '231', '97.9', '2', '0.8', '1', '0.4', '2', '0.8', '0', '0.0', '4', '1.7', '236', '100.0', '2', '0.8', '1', '0.4', '0', '0.0', '0', '0.0', '1', '0.4', '234', '99.2', '236', '100.0', '2', '0.8', '2', '0.8', '0', '0.0', '0', '0.0', '0', '0.0', '0', '0.0', '0', '0.0', '0', '0.0', '234', '99.2', '225', '95.3', '2', '0.8', '0', '0.0', '2', '0.8', '0', '0.0', '1', '0.4', '4', '1.7', '236', '100.0', '236', '100.0', '120', '50.8', '60', '25.4', '42', '17.8', '37', '15.7', '6', '2.5', '3', '1.3', '0', '0.0', '8', '3.4', '0', '0.0', '3', '1.3', '7', '3.0', '0', '0.0', '0', '0.0', '0', '0.0', '0', '0.0', '0', '0.0', '0', '0.0', '0', '0.0', '120', '100.0', '65', '54.2', '20', '16.7', '60', '50.0', '19', '15.8', '2', '1.7', '0', '0.0', '3', '2.5', '1', '0.8', '55', '45.8', '47', '39.2', '27', '22.5', '4', '3.3', '20', '16.7', '8', '6.7', '23', '19.2', '34', '28.3', '1.97', ' ( X ) ', '2.66', ' ( X ) ', '660', '100.0', '120', '18.2', '540', '81.8', '4', '0.6', '1', '0.2', '4', '0.6', '1', '0.2', '527', '79.8', '3', '0.5', '6.3', ' ( X ) ', '6.0', ' ( X ) ', '120', '100.0', '58', '48.3', '116', ' ( X ) ', '2.00', ' ( X ) ', '62', '51.7', '120', ' ( X ) ', '1.94', ' ( X ) ']\n"
     ]
    }
   ],
   "source": [
    "# let's explore it for a few moments\n",
    "print(census_struc[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4-MOVING FROM NOTEBOOKS TO \\*.py FILES**\n",
    "But before you go to do that, we need to start moving the findings of our exploration into a dedicated python file for cleaning the Census data. We're making this migration because notebooks are great for exploring data, but as our files and project grow larger, it is simpler to run the Python files outside notebooks AND sometimes very large files can cause our notebooks to crash.\n",
    "\n",
    "Here's what that new Python file needs to do:\n",
    "1. read the census data file into a variable\n",
    "2. clean the data by removing the \"...,...\" problem using a UDF\n",
    "3. create a nested data structure\n",
    "4. remove unwanted columns\n",
    "5. create a new string from the nested structure\n",
    "6. write the file to disk\n",
    "\n",
    "We've already #1, #2, #3, and #6 together, so we'll isolate those below along with comments to do the other work before putting in its own file.\n",
    "\n",
    "And recall that our UDFs has to be put ahead of the main program to work because the Main Program needs to know what happens in those UDFs.\n",
    "\n",
    "Below is the exact code that will be put into its own file named `clean_census.py` and we'll run it together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "#@@@@@  UDF unquotable() @@@@@@@@@@@@@@@\n",
    "# removes pesky , within \"\" values before changing , to \\t in program\n",
    "def unquotable(input_string):\n",
    "    \n",
    "    # remove \" char \n",
    "    # we remove it by splitting the input string into a list using that char\n",
    "    tmp_list = input_string.split('\"')\n",
    " \n",
    "    # remove , char\n",
    "    for i in range(len(tmp_list)):\n",
    "        # skip items that start/end with commas as these aren't quoted items\n",
    "        if (tmp_list[i][0] == \",\") or (tmp_list[i][-1] == \",\"):\n",
    "            continue\n",
    "        # replace , with - in quoted items\n",
    "        else:\n",
    "            tmp_list[i] = tmp_list[i].replace(\",\",\"-\")\n",
    "\n",
    "    # rejoin items into a string\n",
    "    output = \",\".join(tmp_list)\n",
    "\n",
    "    # get rid of any \",,\" and \",,,\" as it will add extra columns\n",
    "    output = output.replace(\",,,\",\",\")\n",
    "    output = output.replace(\",,\",\",\")\n",
    "    \n",
    "    # return string\n",
    "    return output\n",
    "\n",
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@    \n",
    "#@@@@@@@@ UDF nester() @@@@@@@@@\n",
    "# take a dataset string where each row is separated by input_row_delim \n",
    "# and each column is separate by input_col_delim to create a nested object of lists\n",
    "def nester(input_string,input_row_delim,input_col_delim):\n",
    "\n",
    "    # create a list item for each row in the file using the row delimiter\n",
    "    row_list = input_string.split(input_row_delim)\n",
    "\n",
    "    #output var\n",
    "    nested_data = []\n",
    "    \n",
    "    # created nested structure to store each column separately\n",
    "    # list of rows where each row is a list of columns)\n",
    "    for i in range(len(row_list)): \n",
    "        row = row_list[i]\n",
    "        col = row.split(input_col_delim)\n",
    "        nested_data.append(col)\n",
    "    \n",
    "    # return the nested structure\n",
    "    return nested_data\n",
    "\n",
    "\n",
    "################################################################\n",
    "##### MAIN PROGRAM #############################################\n",
    "################################################################\n",
    "\n",
    "# 1. read the census data file into a variable\n",
    "file_handle = open('C:\\\\Users\\\\colling\\\\!dwd_spring2019\\\\classes\\\\class7\\\\raw_census_2010.csv',\"r\")\n",
    "census_data = file_handle.read()\n",
    "file_handle.close()\n",
    "\n",
    "# 2. clean the data by removing the \"...,...\" problem using a UDF\n",
    "clean_census_data = unquotable(census_data)\n",
    "\n",
    "# 3. create a nested data structure with a UDF\n",
    "nested_census_data = nester(clean_census_data,\"\\n\",\",\")\n",
    "\n",
    "# 4. remove unwanted columns using a UDF\n",
    "# HOMEWORK\n",
    "\n",
    "# 5. create a new string from the nested structure using a UDF\n",
    "# HOMEWORK\n",
    "\n",
    "# 6. write the file to disk\n",
    "file_handle = open('C:\\\\Users\\\\colling\\\\!dwd_spring2019\\\\classes\\\\class7\\\\clean_census_2010.csv',\"w\")\n",
    "file_handle.write('final data var from step #5 goes here')\n",
    "file_handle.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
